	1. OpenShift (Kubernetes + Cloud Provider Feature + enterprise enhancements)
	OpenShift is a container application platform built by Red Hat. It will provide enterprise enhancements on top of Kubernetes( its core engine). It is based on Kubernetes, but provides many additional enterprise features such as:
		a. A full web console
		b. Built‑in CI/CD pipelines (Tekton/BuildConfig)
		c. Strong security (SecurityContextConstraints)
		d. Centralized monitoring and logging
		e. Automated installation, upgrades, scaling(Manual option is also available)
	
	2. What is Kubernetes?
	Kubernetes (K8s) is an open‑source container orchestration system. It automatically handles:
		a. Deploying Containers
		b. Scaling up/down
		c. Restarting failed applications
		d. Networking between components
		e. Storing secrets & configs
		f. Load balancing traffic
	OpenShift internally uses Kubernetes as its core engine.
	
	3. How Kubernetes, Cloud Provider(EKS, AKLS, GKE) and OpenShift is different with each other's.
	Kubernetes (K8s) is just the core engine. If you install raw Kubernetes (Kubeadm, Kops, Minikube), you get:
		a. API Server
		b. Scheduler
		c. Controller Manager
		d. Etcd database
		e. Nodes
		f. Deployment/Pods/Services
	But you don't get:
		a. GUI
		b. Built-in logging
		c. Built-in monitoring
		d. Built‑in security policies
		e. Automatic upgrades
		f. CI/CD
		g. Registry
		h. Enterprise support
	This is where cloud providers & OpenShift come in.
	What Cloud Providers Add on Top of Kubernetes:
		a. Managed Control Plane: You don't manage API server,etcd, scheduler,controller manager. Cloud provider handles upgrades, patching, scaling.
		b. Built-in auto-scaling: Examples: Cluster Autoscaler, Node Auto Provisioning, Horizontal Pod Autoscaler (with cloud metrics)
		c. Network integrations: AWS VPC CNI (for EKS), Azure CNI (for AKS), Google VPC-native mode (for GKE). These give better integration with cloud networks.
		d. Load balancers, storage, and identity integrations: Cloud provider gives out-of-the-box: Cloud Load Balancer, Cloud Storage (EBS, Azure Disk, Persistent Disk), Cloud Identity (IAM, AD, etc.)
		e. Security tools: 
			• IAM integration
			• Cloud security policies
			• Secrets encryption
			• Role-based access control pre-configured
		f. Monitoring & Logging: Cloud providers integrate Kubernetes with: CloudWatch (EKS), Azure Monitor (AKS), Google Cloud Logging/Monitoring (GKE)
		g. Automatic Upgrades: They upgrade the control plane and often the worker nodes.
	So, cloud providers add features but still they don't add extra enterprise features are what OpenShift offers. Those extra features are:
		a. CI/CD platform
		b. Built‑in image registry
		c. Enterprise security policies (SCC, RBAC templates)
		d. Developers' self-service tools
		e. Built-in GitOps
		f. OperatorHub
		g. Route/Ingress controller pre-built
		h. Enterprise pipeline system
		i. Built‑in service mesh
		j. Quotas, project isolation
		k. Web console with admin/developer views
		l. Internal private registry
		m. Enterprise logging/monitoring stack
	
	4. Explain the concepts of Kubernetes?
	The essential concepts of Kubernetes are:
		a. Pod: The smallest deployable unit in Kubernetes. A Pod typically contains one container, sometimes more. Kubernetes never runs containers.Example: A pod running your Node.js app.directly — it runs Pods.
		
		b. Deployment: A controller that ensures the desired number of Pods are always running. Functions of deployment:
			i. Create Pods
			ii. Recreates pods if they crash
			iii. Manages rolling updates
			iv. Manages rollbacks.
		If you want 3 replicas of your app, Deployment ensures exactly 3 pods always run.
		
		c. Service: Pods come and go, so their IPs change. A Service provides a stable IP and DNS name. Types:
			i. ClusterIP(internal)
			ii. NodePort(external)
			iii. LoadBalancer(cloud external)
		Services make your application reachable inside or outside the cluster.
		
		d. Ingress: Inside Kubernetes or OpenShift, pods runs your app and services expose your app inside the cluster but apps are NOT reachable from outside unless we explicitly expose them. So to expose apps to the internet, we need:
			i. Ingress(Kubernetes): An Ingress is a Kubernetes object that defines:
				1) Domain names(hostnames)
				2) Paths(URLs)
				3) Routing rules
				4) TLS(HTTPS) settings
			Ingress does NOT work alone; it needs an Ingress Controller. Common controllers:
				1) NGINX Ingress Controller
				2) Traefik
				3) HAProxy
				4) Cloud-specific ones(AWS, GCP, Azure)
			The controller accepts incoming traffic → applies Ingress rules → sends traffic to the correct Service → which then sends it to Pods.
			A Real Example	Kubernetes Ingress Example
				
			apiVersion: v1	apiVersion: networking.k8s.io/v1
			kind: Service	kind: Ingress
			metadata:	metadata:
			  name: myapp-service	  name: myapp-ingress
			spec:	spec:
			  selector:	  rules:
			    app: myapp	  - host: myapp.example.com
			  ports:	    http:
			    - port: 80	      paths:
			      targetPort: 8080	      - path: /
				        pathType: Prefix
				        backend:
				          service:
				            name: myapp-service
				            port:
				              number: 80
			
			ii. Route(OpenShift): OpenShift has a built‑in alternative called Route. A Route is easier than Ingress because:
				1) No need to install controllers → OpenShift has one built in
				2) TLS, certificates, rewrites → already supported
				3) Integrated with OpenShift Router (HAProxy)
				4) Flow for Route: Internet → OpenShift Router → Route → Service → Pod
			OpenShift Router automatically:
				1) Watches for new Routes
				2) Configures itself dynamically
				3) Handles HTTPS termination
				4) Supports wildcard domains
			OpenShift Route Example	
				• host: myapp.example.com = external domain
			apiVersion: route.openshift.io/v1	• Route automatically integrates with HAProxy router
			kind: Route	• TLS termination (“edge”) means SSL is terminated at router
			metadata:
			  name: myapp-route
			spec:
			  host: myapp.example.com
			  to:
			    kind: Service
			    name: myapp-service
			  port:
			    targetPort: 80
			  tls:
			    termination: edge
		So, Steps to work with:
			i. Kubernetes path:
				1) Install NGINX Ingress Controller
				2) Create an Ingress resource
				3) Configure DNS → Ingress Controller LoadBalancer
				4) App becomes accessible
			ii. OpenShift path:
				1) Create a Route(creating a Kubernetes-style YAML object that OpenShift understands. Example: kind: Route)
				2) OpenShift automatically updates HAProxy router
				3) Configure DNS → OpenShift router
				4) App becomes accessible
		These two work almost the same way — but OpenShift Routes are simpler and more built‑in.
		
		e. ConfigMap: A ConfigMap is a Kubernetes/OpenShift object used to store non‑sensitive configuration data that an application needs to run. ConfigMaps allow us to keep configuration separate from application code. It stores things like:
			i. Application URLs
			ii. File Paths
			iii. Environment variables
			iv. Port numbers
			v. Feature flags
			vi. Any settings that are not secret
		Benefits:
			i. We don’t hard‑code configuration inside the app
			ii. We don’t need to rebuild the container/image when configuration changes
			iii. We can update configuration quickly and safely
			iv. The same application image can be used in dev, test, and prod with different configs
		Key Idea: Application code stays inside the container. Configuration stays outside the container.
		
		f. Secret: A Secret is a special Kubernetes/OpenShift object used to store sensitive information — information that must be protected. Secrets store data such as:
			i. Passwords
			ii. API keys
			iii. Tokens
			iv. SSH keys
			v. TLS certificates
		These are things that should never be visible in plain text or hard-coded in your application.
		We can't store this in configMap because configMap is not secure and anythings inside a configMap is stored as plain text. If someone gets access to the ConfigMap: They see your passwords.
		How Secrets protect data(Base64 encoding):  The data in Secrets is stored in Base64 encoded form, not plain text. Even though Base64 is not strong encryption, it hides the actual value from basic viewers. Example: Password: admin123 Stored like: YWRtaW4xMjM=
		Secrets are kept in a more secure storage area in etcd (the Kubernetes database). OpenShift automatically: enables encryption-at-rest, restricts access, manages permissions. Only users with the correct RBAC permissions can read Secrets. Secrets never appear directly in pod logs. Secrets can be injected into pods as:
			i. Environment variables
			ii. Mounted files
			iii. Certificates. 
		
		g. CNI(Container Network Interface): Provides networking to Kubernetes pods. It decides: how pod IPs are assigned, how pods communicate and network routing. Examples: Calico, Flannel, OVN‑Kubernetes.
		
		h. CR/CRD(Custom Resource/Custom Resource Definition): Allows Kubernetes to be extended with new resource types. Example: OpenShift adds Route, BuildConfig, Operators add CRDs like Database, KafkaCluster. CRDs make Kubernetes highly customizable.
		
		
	5. What is Rancher?
	Rancher is a Kubernetes orchestration tool, which manages Kubernetes clusters from one or multiple providers. It can manage ANY distribution (AKS, EKS, GKE, RKE, K3s). Rancher is designed to be a multi‑cluster management platform. According to SUSE Rancher documentation: “Rancher users have the choice of creating Kubernetes clusters with RKE or cloud Kubernetes services such as GKE, AKS, and EKS. Rancher users can also import and manage their existing Kubernetes clusters created using any Kubernetes distribution or installer.”
	Rancher explicitly supports: Amazon EKSm, Azure AKS, Google GKE. The Rancher documentation states: Rancher can manage “EKS, GKE and AKS clusters” as well as self‑managed clusters. This confirms that Rancher manages managed Kubernetes services from the major cloud providers.
	
	
	6. Explain OpenShift Deployment Models?
	OpenShift can be installed and used in different ways, depending on whether you are an enterprise, developer, or cloud customer. OpenShift has two major models:
		a. Self‑Managed OpenShift (Enterprise installs and manages everything): This means your company installs OpenShift itself on physical servers or VMs. OpenShift self-managed is heavy, enterprise‑grade, and used in banking, telecom, etc.
			i. Flow: OpenShift → Self-managed → Installer → RHCOS → Admin → RHEL, RHCOS
				1) Installer: You download OpenShift installer from Red Hat.
				2) RHCOS(Red Hat CoreOS):  Primary OS for OpenShift nodes.
				3) Admin responsibility – You manage:
					a) API Server
					b) Etcd
					c) Control Plane
					d) Worker Nodes
					e) Security
					f) Upgrades
					g) Logging/Monitoring
				4) RHEL / RHCOS – Nodes can be RHCOS or RHEL depending on setup.
		
		b. Managed OpenShift (Cloud providers manage control plane): In this model, cloud providers run OpenShift for you. Cloud Managed OpenShift Services:
			i. ROSA → Red Hat OpenShift Service on AWS
			ii. ARO → Azure Red Hat OpenShift
			iii. ROKS → Red Hat OpenShift on IBM Cloud
			iv. GCP → Partners provide managed OpenShift solutions
		Cloud provider manages:
			i. API Server
			ii. Etcd
			iii. Scheduler
			iv. Controller
			v. Patching
			vi. Control panel scaling
		You only Manage:
			i. Worker node
			ii. Applications
			iii. Networking inside cluster
			
			
	7. Why OpenShift Is a Better Choice Than Cloud‑Provided Kubernetes (EKS / AKS / GKE).
	Cloud‑provider Kubernetes services (EKS, AKS, GKE) provide only a managed Kubernetes control plane along with basic cloud integrations such as load balancers, storage, IAM, and networking.
	OpenShift, on the other hand, provides a full enterprise-grade application platform built on top of Kubernetes. It includes many additional built-in capabilities—such as CI/CD, registry, monitoring, logging, developer tools, security policies, multi‑tenancy, Operators, and GitOps—that cloud Kubernetes services do not provide by default.
	
	8. Explain OpenShift Enterprise Features.
	OpenShift Enterprise Features is:
		a. OLM (Operator Lifecycle Manager): It is a component of OpenShift used to:
			i. Install Operators
			ii. Update Operators
			iii. Manage Operators versions
			iv. Manage Operator dependencies
			v. Provide OperatorHub catalog
		Think of OLM as the package manager for Kubernetes operators (like App Store for operators).
		
		b. What is an Operator: An Operator is Kubernetes automation software that encodes human operational knowledge into code. An Operator:
			i. Watches the cluster
			ii. Reads your desired configuration (CRDs)
			iii. Takes actions automatically
			iv. Ensures the application stays healthy
			v. Example: ElasticSearch Operator, Kafka Operator, Istio Operator, ArgoCD Operator
		
		c. Controllers (Desired-state management): A Controller is a control-loop program inside Kubernetes. It constantly checks: Actual State vs Desired State And if something is wrong, it fixes it automatically. Meaning controllers read:
			i. YAML Manifests
			ii. Helm Charts
			iii. CRDs from Operators
		And then keep the system in desired state. This is the core of Kubernetes self-healing.
		
		d. Helm, Manifests, Operators comparison
			i. Manifests: Normal YAML files (Deployment, Service, etc.)
			ii. Helm Charts:  Package manager for Kubernetes; bundles many YAMLs.
			iii. Operators: CRDs + controllers that understand complex apps.
			
		e. ArgoCD (GitOps): 
		ArgoCD is a GitOps tool that continuously synchronizes the cluster state with the desired state stored in a Git repository. It ensures that whatever is defined in Git is exactly what runs in the Kubernetes/OpenShift cluster. ArgoCD Workflow:
			i. ArgoCD Operator
				1) ArgoCD in OpenShift is installed and managed using an Operator
				2) The Operator handles installation, upgrades, configuration, and lifecycle of ArgoCD.

			i. ArgoCD Controller
				1) The ArgoCD Controller is a control loop that continuously
					a) Watches the Git repository
					b) Reads manifests / Helm charts / configuration files
					c) Compares Git state with cluster state
					
		If Git and the cluster do not match, ArgoCD takes corrective action. ArgoCD uses Kubernetes objects like ConfigMaps and CRDs to store:
			i. Application configuration
			ii. Sync policies
			iii. Connection details
			iv. Desired state definitions
		When Git changes, ArgoCD updates the ConfigMap/CRD entries to reflect the new desired state.
		
		f. Auto-Healing/ Desired State Enforcement/ GitOps Reconciliation: 
		If someone manually edits or breaks something inside the cluster, Example: 
			i. A pod is deleted
			ii. A deployment is changed manually
			iii. A configmap is modified
			iv. Someone scales down replicas manually
		ArgoCD will: Detect the drift → Compare with Git → Fix the cluster → Restore Git-defined state
		This process is called Auto-Healing/ Desired State Enforcement/ GitOps Reconciliation
		
