=> What is Blob Storage: Blob Storage is a type of object storage used to store large amounts of unstructured data such as images, videos, documents, backups, and media files.
                         Blob stands for Binary Large Object, which means data stored in binary form (files).
                         Blob Storage is not a database and not a traditional file system.

=> What kind of data is stored in Blob Storage?
  -> Blob Storage is used to store files, not structured records. Example: Images (jpg, png), Videos (mp4), PDFs, Word, Excel files, Log files, Backups, Static website files
  -> It is not suitable for Tables, Rows and Columns, Transactional Business data

=> How data is organized in Blob Storage
  -> Blob Storage follows a three-level hierarchy: storage account --> Container --> Blob(Files)
  -> Example: company-storage --> invoice_101.pdf --> invoice_102.pdf
      -> Storage Account: Top-level storage resource
      -> Container: Logical grouping of blobs (similar to folder)
      -> Blob: Actual file (image, video, document, etc.)

=> What is Object Storage?
  -> Blob Storage is an example of object storage, where:
    -> Each file is stored as a single object
    -> Objects are accessed using URLs or APIs
    -> There are no real directories like a file system
    -> Each Object contain Data (file content),  Metadata (file info), Unique identifier (blob name / URL)

=> SMB (Server Message Block) vs NFS (Network File System)
  -> SMB (Server Message Block)
    -> A network file sharing protocol mainly used by Windows systems.
    -> Allows applications to read, write, create, and delete files on a remote server as if they are local.
    -> Supports file locking, authentication, and permissions.
    -> Commonly used for shared folders and network drives.
    -> In Azure Files, SMB is used to mount file shares on Windows, Linux, macOS.
    -> Example: Mapping a network drive like Z: in Windows.
  -> NFS (Network File System)
    -> A network file sharing protocol mainly used by Linux and Unix systems.
    -> Allows multiple machines to access shared files over a network.
    -> Designed for high performance and scalability in Linux environments.
    -> Uses a stateless or lightweight state model depending on version.
    -> In Azure Files, NFS is used to mount file shares on Linux systems.
    -> Example: Mounting a shared directory like /mnt/shared.

=> What is Azure Blob Storage?
  -> Azure Blob Storage is Microsoft’s object storage solution for the cloud. It is optimized for storing massive amounts of unstructured data. 
  -> Unstructured data is data that does not follow a predefined data model or schema, such as: Text files, Images, Videos, Binary files

=> What is Azure Blob Storage used for?
  -> Azure Blob Storage is designed for the following use cases:
    -> Serving images or documents directly to a browser
    -> Storing files for distributed access
    -> Streaming video and audio content
    -> Writing and storing log files
    -> Backup, restore, disaster recovery, and archiving
    -> Storing data for analysis by on-premises or Azure-hosted services

=> How can Blob Storage be accessed?
  -> Objects stored in Azure Blob Storage can be accessed from anywhere in the world using HTTP or HTTPS. Access methods include:
    -> Azure Storage REST API
    -> Azure PowerShell
    -> Azure CLI
    -> Azure Storage client libraries (Java, .NET, Python, etc.)

=> Storage Tier: Storage tier defines the cost and access characteristics of data based on how frequently it is accessed. There are basically 3 types pf tier:
  -> Hot Tier:
    -> Used for data that is accessed frequently.
    -> Has the highest storage cost but the lowest access cost.
    -> Suitable for active data like images, videos, or files used by applications.
  -> Cool Tier:
    -> Used for data that is accessed infrequently (at least once in 30 days)
    -> Has lower storage cost than Hot tier but higher access cost.
    -> Suitable for backups, older documents, or infrequently used data.
  -> Archive Tier:
    -> Used for data that is rarely accessed (long-term storage).
    -> Has the lowest storage cost but the highest access cost and retrieval latency.
    -> This access tier is considered to be offline and can't be read or modified.
    -> Data must be rehydrated before access.
    -> Suitable for compliance data, long-term backups, and archival records.
  Storage tiers apply only to Standard storage accounts(only in Blob Storage not in Azure Files, Queue Storage, or Table Storage), while Premium storage accounts always provide high performance and do not support Hot, Cool, or Archive tiers.

=> Types of Azure Storage Accounts
  -> Azure Storage provides two performance levels Storage Accounts:
    -> Standard: This is the default and recommended storage account type for most Azure Storage scenarios.
      -> Uses hard disk drives (HDD)
      -> Cost-effective
      -> Recommended for most use cases
      -> Supported services:
        -> Blob Storage (including Azure Data Lake Storage Gen2):
          -> It support storage tier: Hot, Cool and Archive
          -> It is designed for storing large amounts of unstructured data (any kind of files).
          -> Files are stored as objects (blobs) inside containers.
          -> Accessed and uploaded using URL / REST API / SDKs.
          -> Not mounted like a traditional file system.
          -> Optimized for high throughput and scalability.
          -> Example: images, videos, PDFs, backups, logs, big data files.
        -> Azure Files
          -> Provides fully managed file shares in the cloud.
          -> Supports standard file system access using SMB and NFS protocols.
          -> Used when applications need shared file access, similar to a network file system.
          -> This is file system storage, access like a network drive. 
          -> Can be mounted on Windows, Linux, or macOS machines.
          -> You can mount it locally using PowerShell or shell script, and it appears as a new drive.
          -> Any upload, delete, or update reflects immediately in the Azure portal.
          -> Example: shared application files, configuration files, lift-and-shift file servers.
        -> Queue Storage
          -> Used for storing small pieces of data called messages.
          -> Designed for scalable and asynchronous communication between application components.
          -> Helps decouple different parts of an application.
          -> Examples: background job processing, task queues, message buffering.
        -> Table Storage
          -> Stores semi-structured data in the form of key-value pairs.
          -> Data is stored in tables, but there is no relationship between tables.
          -> Data is accessed using Partition Key and Row Key.
          -> Designed for fast access to large amounts of structured or semi-structured data.
          -> Examples: user profiles, metadata storage, configuration data.
      -> Redundancy options:
        -> LRS (Locally Redundant Storage)
        -> ZRS (Zone-Redundant Storage)
        -> GRS (Geo-Redundant Storage)
        -> RA-GRS (Read-Access Geo-Redundant Storage)
        -> GZRS (Geo-Zone-Redundant Storage)
        -> RA-GZRS (Read-Access Geo-Zone-Redundant Storage)
      -> Usage:
        -> Standard storage account for blobs, file shares, queues, and tables
        -> Recommended for most Azure Storage workloads
    -> Premium: Premium Storage Accounts are designed to deliver high performance and low latency by using solid-state drives (SSDs) instead of traditional hard disk drives (HDDs). 
      -> Uses solid-state drives (SSD)
      -> Higher performance and lower latency
      -> Higher cost
      -> Designed for high-performance workloads
      -> Azure provides three types of premium storage accounts, each optimized for a specific storage service and workload.
        -> Premium Block Blobs
          -> Premium Block Blob Storage is optimized for workloads that require high transaction rates, low latency, and frequent access to blob data.
          -> This storage account type is ideal for storing block blobs and append blobs, which are commonly used for files such as images, documents, logs, and analytics data.
          -> It also supports Azure Data Lake Storage Gen2, making it suitable for big data analytics and data processing scenarios.
          -> Supported services: Block blobs, Append Blobs, Azure Data Lake Storage Gen2
          -> Redundancy Options: 
            -> Locally Redundant Storage (LRS)
            -> Zone-Redundant Storage (ZRS)
          -> Typical Use Cases
            -> Applications with high read/write transaction rates
            -> Workloads involving smaller objects accessed frequently
            -> Applications that require consistently low latency
            -> Real-time analytics and streaming ingestion scenarios
        -> Premium File Shares
          -> Premium File Shares Storage is designed to provide high-performance file storage using the Azure Files service.
          -> This storage account type is optimized for enterprise-grade file system workloads, where applications require low-latency access to shared files using standard file system protocols.
          -> It is particularly recommended when Network File System (NFS) support is required in Azure Files.
          -> upported Services: Azure Files only
          -> Redundancy Options
            -> Locally Redundant Storage (LRS)
            -> Zone-Redundant Storage (ZRS)
          -> Typical Use Cases
            -> Enterprise applications requiring shared file systems
            -> High-performance file shares for application data
            -> Lift-and-shift workloads migrating on-prem file servers to Azure
            -> Scenarios requiring NFS-based file access
        -> Premium Page Blob Storage Account: 
          -> Premium Page Blob Storage is designed specifically for page blob workloads, which require random read and write operations with very low latency.
          -> Page blobs are primarily used to store virtual hard disk (VHD) files, making this storage account type ideal for Azure virtual machine disks and other disk-based workloads.
          -> Supported Services: Page Blobs only
          -> Redundancy Options
            -> Locally Redundant Storage (LRS)
            -> Zone-Redundant Storage (ZRS)
          -> Typical Use Cases
            -> Virtual machine disks
            -> High-performance random I/O workloads
            -> Scenarios requiring frequent updates to specific ranges within a file

=> Types of Redundancy in Azure
  -> LRS – Locally Redundant Storage: It means 3 copies of data stored in the same data centre within the same region.
  -> ZRS – Zone-Redundant Storage: It means 3 copies of data stored across different availability zones within the same region.
  -> GRS – Geo-Redundant Storage: It means 3 copies of data stored in the primary region and 3 copies of data stored in a paired secondary region (different region).
  -> RA-GRS – Read-Access Geo-Redundant Storage: It means you can read data from the secondary region even when the primary region is healthy. Used when global read access is required.
  -> GZRS – Geo-Zone-Redundant Storage: It means ZRS in the primary region and GRS replication to a secondary region.
  -> RA-GZRS – Read-Access Geo-Zone-Redundant Storage: It means GZRS with read access enabled for the secondary region. Provides the highest availability and has the highest cost.

=> Explain Type of Blobs in Azure?
  -> There are three different types of Blobs in Azure:
    -> Block Blobs: Upload / Replace whole file
      -> Store text and binary data. Example: Streaming Content, Images or Videos
      -> Made up of blocks of data that can be uploaded and managed independently.
      -> Optimized for streaming and storing large files. Maximum size is up to ~190.7 TiB.
      -> Commonly used for images, videos, documents, backups, logs.
      -> Once uploaded, a block blob cannot be modified partially (like editing a few bytes in the middle).
      -> You can overwrite the entire blob or delete and re-upload it.
      -> You can also upload it block-by-block, but once committed, random in-place modification is not supported.
    -> Append Blobs: Only add at the end
      -> Designed specifically for append-only operations. Example Logs files
      -> You cannot overwrite existing content in the file.
      -> You can only append new data at the end of the same blob.
      -> Used mainly for logs, audit files, and streaming data.
    -> Page Blobs: Works like a disk for VM
      -> Random read and write operations. Example Virtual Machine Disks
      -> Used to store Virtual Hard Disks (VHDs) for Azure Virtual Machines.
      -> Supports random read and write operations.
      -> Acts like a disk, not a directory itself.
      -> When attached to a VM, the operating system formats it and then it appears as drives like C:, D: etc (similar to your local machine)

=> Explain public access level in Azure Blob Storage?
  -> Public access level controls who can read blobs without authentication. These access levels are set at the container level, not per storage account.
  -> There are 3 different access levels:
    -> Private (No public access)
      -> No anonymous access allowed
      -> Only accessible using: Azure AD, SAS token, Storage account key
      -> Blobs and container are both private
      -> Use cases: User documents, Internal application data, Sensitive files
    -> Blob (Anonymous read access for blobs only)
      -> Public users can read individual blobs
      -> Container listing is NOT allowed
      -> If someone has the exact blob URL, they can access it
      -> Upload / delete still requires authentication
      -> Use cases: Public images, PDFs shared via direct link, Public media files
    -> Container (Anonymous read access for container and blobs)
      -> Public users can: List blobs in the container and read blobs
      -> Anyone can browse all files if container URL is known
      -> Use cases: Public datasets, Open data sharing, Static website assets (older approach)

=> Azure Storage Security – Encryption
  -> Azure Storage secures data mainly using encryption. Encryption ensures that even if someone gets physical access to disks, data is unreadable.
  -> There are three different type of encryption which Azure is using:
    -> Service-Side Encryption (SSE) – Default & Most Important
      -> It means Data is encrypted automatically by Azure when it is stored and decrypted automatically when accessed
      -> It happens inside Azure, not in your application. You don’t write any code for this.
      -> It Uses AES-256 encryption and Enabled by default for all storage accounts and Cannot be disabled and No extra cost
      -> What is encrypted: Blobs (Block, Append, Page – even Archive tier), Files (Azure Files), Queues, Tables, Disks, Metadata, Primary + Secondary regions (GRS, GZRS)
      -> Encryption is independent of: Standard / Premium, Hot / Cool / Archive, LRS / ZRS / GRS
      -> Think of it like BitLocker for Azure Storage disks. You store data → Azure encrypts it → you read data → Azure decrypts it.
    -> Encryption Key Management (Who controls the key?)
      -> Microsoft-Managed Keys (Default)
        -> Azure creates, stores, rotates and manages encryption keys
        -> Supported for all Azure Storage services
        -> Used when no special compliance requirement  
      -> Customer-Managed Keys (CMK)
        -> Keys are stored in Azure Key Vault or Key Vault HSM
        -> Customer controls key rotation, access and revocation
        -> Supported for Blob Storage and Azure Files
      -> Customer-Provided Keys (CPK)
        -> Customer provides encryption key with every request
        -> Azure never stores the key
        -> Supported only for Blob Storage
    -> Client-Side Encryption (Optional, Extra Security)
      -> Data is encrypted in the client application before uploading to Azure
      -> Azure stores already-encrypted data and never sees plain text
      -> Supported by Blob Storage and Queue Storage client libraries
      -> Uses AES encryption
      -> Version 1 uses AES with CBC mode
      -> Version 2 uses AES with GCM mode (more secure and recommended)

=> Explain Blob Subtypes in Azure?
  -> Azure Blob Storage supports three blob subtypes:
    -> Base Blob: 
      -> The main blob (original blob)
      -> Represents the current active version of the blob
      -> This is what you normally upload, read, overwrite, or delete
      -> Can be modified (overwrite entire blob, append, or page write depending on blob type)
      -> Example: resume.pdf stored in a container
    -> Blob Snapshot
      -> A read-only point-in-time copy of a blob
      -> Snapshot is immutable (cannot be modified)
      -> Snapshot has the same name as base blob, but with a timestamp
      -> Stored in the same container
      -> Used mainly for backup and restore
      -> Example: Take snapshot of resume.pdf before overwriting it
    -> Blob Version
      -> Automatically created when blob versioning is enabled
      -> Each update creates a new version with a unique version ID
      -> Versions are read-only
      -> Base blob always points to the latest version
      -> Versioning is managed automatically by Azure
      -> Example: Upload resume.pdf multiple times --> Each upload creates a new version --> You can restore any previous version

=> Azure Blob Storage Lifecycle Management
  -> Lifecycle management automatically manages blob data based on age. Lifecycle rules can apply to Base Blob (current blob), Snapshots, Previous Versions
  -> Helps reduce storage cost by:
    -> Moving data to cheaper tiers
    -> Deleting old data
  -> Defined using a JSON policy
  -> Lifecycle Policy Structure
    -> A lifecycle policy is a collection of rules. Each rule has:
      -> Filter set (which blobs the rule applies to)
      -> Action set (what to do with those blobs)
  -> Rule Components: 
    -> Rule Metadata
      -> name → Unique rule name
      -> enabled → Enable / disable rule
      -> type → Always Lifecycle
      -> definition → Contains filters + actions
    -> Filters (Which blobs?): Filters limit the scope of the rule. Common Filters
      -> blobTypes (Required)
        -> blockBlob (most common)
      -> prefixMatch
        -> Apply rule to blobs with specific path
        -> Must start with container name
      -> blobIndexMatch
        -> Match blobs using index tags
    -> Actions (What to do?): Actions are applied when age conditions are met. Supported actions
      -> Tiering actions
        -> tierToHot
        -> tierToCool
        -> tierToCold
        -> tierToArchive
      -> Deletion actions
        -> delete base blob
        -> delete snapshot
        -> delete previous versions
  -> Azure calculates age using timestamps:
    -> Base blob: Uses last modified time
    -> Snapshot: Uses snapshot creation time
    -> Versions: Uses version creation time

  -> Common Run Conditions
    -> daysAfterModificationGreaterThan: Used for base blobs
    -> daysAfterCreationGreaterThan: Used for snapshots & versions
    -> daysAfterLastAccessTimeGreaterThan: Used when access tracking is enabled
    -> daysAfterLastTierChangeGreaterThan: Prevents immediate re-archiving after rehydration
  -> If multiple actions apply to the same blob: Azure applies the cheapest action. Cost order (cheapest → expensive): Delete → Archive → Cold → Cool → Hot

=> How to Implement Blob storage lifecycle policies.
  -> You can add, edit, or remove a policy by using any of the following methods: Azure portal, Azure PowerShell, Azure CLI, REST APIs
  -> In Azure Portal, There are two ways to add a policy through the Azure portal: Azure portal List view, and Azure portal Code view. 
  -> Following is an example of how to add a policy in the Azure portal 
    -> In the Azure portal, navigate to your storage account.
    -> Under Data management, select Lifecycle Management to view or change lifecycle management policies.
    -> Select the List View tab.
      -> Used to create and manage rules visually. You can:
        -> Add a new rule
        -> Configure filters (container, prefix, blob type)
        -> Configure actions (tier, delete, snapshots, versions)
      -> Azure generates the JSON automatically in the background
    -> Select the Code View tab. On this tab, you can define a lifecycle management policy in JSON.
      -> Used to view and edit the full lifecycle policy JSON
      -> You do not get an “Add rule” button
      -> You must manually edit the JSON to:
        -> Add new rules
        -> Modify existing rules
        -> Copy / paste rules
      -> Changes are applied when you Save the JSON
  -> Example: The following JSON is an example of a policy that moves a block blob whose name begins with log to the cool tier if it has been more than 30 days since the blob was modified.
      {
        "rules": [
          {
            "enabled": true,
            "name": "move-to-cool",
            "type": "Lifecycle",
            "definition": {
              "actions": {
                "baseBlob": {
                  "tierToCool": {
                    "daysAfterModificationGreaterThan": 30
                  }
                }
              },
              "filters": {
                "blobTypes": [
                  "blockBlob"
                ],
                "prefixMatch": [
                  "sample-container/log"
                ]
              }
            }
          }
        ]
      }

=> Rehydrate Blob Data from Archive Tier
  -> Archive tier blobs are offline. While a blob is in Archive, you can't be read or modify
  -> To access the data, the blob must be rehydrated to an online tier: Hot or Cool
  -> There are two ways to Rehydrate an Archive Blob:
    -> Copy Archived Blob to an Online Tier (Recommended)
      -> Create a new blob in Hot or Cool tier
      -> Source blob remains in Archive
      -> Uses: 
        -> Copy Blob
        -> Copy Blob From URL
      -> You cannot overwrite the same blob
      -> Destination blob must have:
        -> Different name OR Different container
      -> Version note:
        -> Before 2021-02-12 → copy only within same account
        -> After 2021-02-12 → can copy to another account in same region
    -> Change Blob Access Tier (Set Blob Tier)
      -> Change tier from Archive → Hot/Cool
      -> Uses Set Blob Tier operation
      -> Rehydrates the same blob
      -> Important behavior:
        -> Operation cannot be canceled once started
        -> Blob still shows as Archive until rehydration completes
        -> Blob becomes accessible only after completion
      -> Rehydration Time: Rehydration is not instant. It can take several hours. Large blobs are rehydrated more efficiently. Many small blobs may take longer overall
    -> When rehydrating, you can set priority as:
      -> Standard(default)
        -> Processed in request order
        -> Can take up to 15 hours
      -> High
        -> Faster processing
        -> Often completes in under 1 hour
        -> Applies mainly to blobs under 10 GB
      -> You can check priority using: Get Blob Properties

