=> **Azure Function**: Azure Functions is a serverless compute service that enables you to run event‑driven code without having to manage servers. With Azure Functions, you write less code, maintain less infrastructure, and reduce operational costs. Instead of provisioning, configuring, or maintaining servers, Azure automatically provides and manages all the required compute resources to keep your applications running efficiently.

-> It’s important to note that serverless does not mean “there are no servers.” Rather, it means that all server‑related responsibilities—such as provisioning, scaling, patching, and managing infrastructure—are fully handled by the cloud provider. Azure automatically decides when to allocate servers, when to scale out or scale in, and when to clean up resources based on demand.

-> Azure Functions also supports triggers and bindings, which simplify app development. A trigger defines how your function is invoked (for example, an HTTP request, a timer, or a message in a queue), while bindings provide a declarative way to connect inputs and outputs to other Azure services. This allows developers to focus only on writing business logic while Azure handles the rest.
	
=> Differentiate Azure Web Apps vs Azure Functions.
	Both are Azure compute services, but there is a major difference between these services:
		a. Execution Model:
			i. Azure Web Apps (Always Running Application)
				1) Hosts full web applications (API, MVC, Razor Pages, static website, etc.).
				2) Runs continuously, even if no request is happening.
				3) Maintains state (using session, caching, long‑running logic).
			ii. Azure Functions (Serverless, Event‑Driven)
				1) Runs only when a trigger occurs (HTTP request, timer, queue message, etc.).
				2) Stateless: Each execution is independent.
				3) Ideal for small pieces of code that run on demand.
				4) Automatically stops when not in use.
		b. Cost Model
			i. Azure Web Apps
				1) You pay for the Web App Plan (App Service Plan) regardless of usage.
				2) Costs are based on reserved compute resources, not executions.
			ii. Azure Functions
				1) Pay-per-execution (Consumption Plan).
				2) Great when traffic is irregular or unpredictable.
				3) Very cost‑efficient for event-driven workloads.
		c. Scalability
			i. Azure Web Apps
				1) Scaling is manual or rules-based.
				2) Limited by App Service Plan resources.
			ii. Azure Functions
				1) Auto-scales instantly depending on trigger load.
				2) Can scale to thousands of instances automatically.
		d. Development Model
			i. Azure Web Apps
				1) Application‑based architecture.
				2) Supports complex routing, middleware, UI, and API layers.
				3) More control over hosting environment.
			ii. Azure Functions
				1) Function‑based architecture.
				2) Use triggers and bindings to connect services.
				3) Code focused on single responsibility.
		e. Use Case
			i. Azure Web Apps
				1) Full‑fledged websites
				2) REST APIs with predictable traffic
				3) Enterprise applications
				4) Long-running workflows
				5) Applications requiring session/state
			ii. Azure Functions
				1) Background tasks
				2) Scheduled jobs
				3) Queue processing
				4) Lightweight APIs
				5) IoT event handlers
				6) Image processing on upload
		f. Advantages:
			i. Azure Web Apps
				1) Always running: Ideal for applications that require continuous availability.
				2) Supports full web applications: MVC apps, REST APIs, dashboards, websites, etc.
				3) Better control over hosting environment: Custom domains, SSL, scaling rules, deployment slots, logging, etc.
				4) Good for stateful apps: Web Apps can maintain state through session, caching, etc.
				5) Suitable for long-running operations: No strict timeout like Functions.
			ii. Azure Functions
				1) Cost‑effective (Pay‑per‑execution): You only pay when your function runs — perfect for low or unpredictable workloads.
				2) Automatic scaling: Scales instantly based on events. Can scale to thousands of instances.
				3) Event‑driven architecture: Well‑suited for background processing: queues, timers, webhooks, IoT events, etc.
				4) Simplified development: Triggers and bindings connect your function to services with minimal code.
				5) Fast deployment: Small, isolated pieces of code = quicker updates and releases.
		g. Disadvantages:
			i. Azure Web Apps
				1) Higher cost: You pay for the App Service Plan even if the app has zero traffic.
				2) Scaling is slower and not event‑driven: Scaling requires manual steps or auto-scale rules.
				3) More infrastructure responsibility: You need to choose instance size, region, scale count, etc.
				4) Heavier deployment compared to Functions: Complex apps take longer to build, deploy, and manage.
			ii. Azure Functions
				1) Cold start issue (Consumption Plan): If the function is idle for a while, first request can be slow.
				2) Not meant for long-running tasks: Timeout limits apply depending on the plan.
				3) Stateless executions: If you need session/state, you must use external storage.
				4) Less control over server environment: Since it’s fully serverless, configuration options are limited.
				
			
=> **Explain Azure Logic Apps**: It is a cloud based workflow automation tool by Microsoft that lets you connect different apps and services-like SharePoint, Teams, Outlook, and more - without writing any code. You triggers and actions to build visual workflows. For example: "When a file is upload in SharePoint" - that's your trigger. Then "Send a Message in Microsoft Teams" - that's your action. It is a part of Microsoft serverless ecosystem, which means it run automatically in the cloud and you only pay for what you use.
	
=> **Compare Azure Functions and Azure Logic Apps**: Both Functions and Logic Apps are Azure Services that enable serverless workloads. Azure Functions is a serverless compute service, whereas Azure Logic Apps is a serverless workflow integration platform. Both can create complex orchestrations. An orchestration is a collection of functions or steps, called actions in Logic Apps, that are executed to accomplish a complex task.
	For Azure Functions, you develop orchestrations by writing code and using the Durable Functions extension. For Logic Apps, you create orchestrations by using a GUI or editing configuration files.
	
=> **What is Azure WebJobs**: Azure WebJobs is a feature of Azure App Service that allows you to run background tasks or scripts inside the same App Service Plan where your Web App, API, or Mobile App is running. An Azure Web App (App Service) can have one or more WebJobs attached to it. A WebJob can run: .exe, .cmd/.bat, .ps1(PowerShell), .sh(Bash), .py(Python), Node.js scripts, .NET console applications. How it works:
		a. You deploy these scripts/executables into your Web App (typically under wwwroot/App_Data/jobs/…).Azure will then execute the WebJob:
			i. Continously
			ii. Or when triggered(manually or scheduled)
		b. Flow: App Service (Main App) Contains WebJob --> WebJob executes batch/exe/script --> Background task runs
-> The WebJobs SDK adds a programming model similar to Functions—things like triggers (start when a queue message arrives, timer ticks, blob is created) and bindings (easy input/output to services) → so your code stays small and clean. For example: you write a Java code, you use attributes like QueueTrigger, BlobTrigger, TimerTrigger. The SDK connects your code to Azure services automatically. You don’t manually poll queues/blobs or write infrastructure code
	
=> **Compare Functions and WebJobs**: Like Azure Functions, Azure App Service WebJobs with the WebJobs SDK is a code-first integration service that is designed for developers. Both are built on Azure App Service and support features such as source control integration, authentication, and monitoring with Application Insights integration. Azure Functions is built on the WebJobs SDK, so it shares many of the same event triggers and connections to other Azure services. 
	
=> **Azure Functions Hosting Plans**: There are 5 different Azure Functions hosting plan:
		a. Consumption
		b. Flex Consumption
		c. Premium
		d. App Service(Dedicated)
		e. Container Apps
		
=> **What is local.settings.json**: When you run your Azure Function locally (on your machine), it needs configuration—connection strings, API keys, and other app settings. These live in a file called local.settings.json. Think of it as your local “App Settings.” In Azure (the cloud), the same settings live under Configuration → Application settings in your Function App. Many triggers/bindings have a property like connection or connectionStringSetting. You put the name of a setting there (e.g., "AzureWebJobsStorage"), and the runtime will look it up in Values. A complete example of local.setting.json:
		{
		  "IsEncrypted": false,
		  "Values": {
		    "AzureWebJobsStorage": "UseDevelopmentStorage=true",
		    "FUNCTIONS_WORKER_RUNTIME": "dotnet-isolated",
		    "BlobConnection": "DefaultEndpointsProtocol=https;AccountName=youracct;AccountKey=...;EndpointSuffix=core.windows.net",
		    "ServiceBusConnection": "Endpoint=sb://your-namespace.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=...",
		    "CosmosDBConnection": "AccountEndpoint=https://yourcosmos.documents.azure.com:443/;AccountKey=...;"
		  }
		}
		Example: Blob trigger that uses a connection setting. Here, Connection = "BlobConnection" means: look for a key named BlobConnection in Values.
		[Function("BlobTriggered")]
		public static void Run(
		    [BlobTrigger("images/{name}", Connection = "BlobConnection")] Stream blob,
		    string name,
		    ILogger log)
		{
		    log.LogInformation($"Processed blob {name}");
		}
		a. IsEncrypted: When this setting is set to true, all values are encrypted with a local machine key. Default value is false. 
		   You might want to encrypt the local.settings.json file on your local computer when it contains secrets, such as service connection strings. 
           The host automatically decrypts settings when it runs. Use the func settings decrypt command before trying to read locally encrypted settings.

		a. Value: Many triggers and bindings have a property that refers to a connection string app setting, like Connection for the Blob storage trigger. 
		   For these properties, you need an application setting defined in the Values array. Commonly used settings (names you’ll see often):
				i. AzureWebJobsStorage – Storage account for the Functions runtime (required by many triggers and the host).
				ii. FUNCTIONS_WORKER_RUNTIME – E.g., dotnet, dotnet-isolated, node, python.
				iii. BlobConnection, ServiceBusConnection, CosmosDBConnection – Your own friendly names for connection strings used by bindings.
		
=> Explain Trigger and Bindings?
		a. Trigger: Event that make your function start running. For Example: Pressing a button triggers a lift, Opening a door triggers an automatic light, 
		   Uploading a file triggers a virus scanner, Receiving a WhatsApp message triggers a notification sound. In Azure, Every function has one and only one trigger. Type of Trigger in Azure:
				i. HTTP Trigger: An HTTP request Hit an endpoint.
				ii. Blob Trigger: A file is uploaded to Blob storage
				iii. Queue Trigger: A message arrives in a queue
				iv. Service Bus Trigger: A message arrives in service bus.
				v. Cosmos DB Trigger: A document is updated in cosmos DB
				vi. Time Trigger: Time reaches X:00 PM
		   To create a Trigger function, follow the three step:
			i. Which annotation is used. It will tell you the trigger type?
				1) @BlobTrigger → fires when a blob changes
				2) @QueueTrigger → fires when a queue message appears
				3) @HttpTrigger → fires when HTTP request is made
				4) @TimerTrigger → fires on CRON schedule
				5) @CosmosDBTrigger → fires when a doc changes
				6) @ServiceBusQueueTrigger → fires on SB message
			ii. What’s inside the annotation. This tells you how to configure the trigger. ?
				1) path → where blobs are
				2) queueName → name of queue
				3) schedule → timer CRON
				4) databaseName, containerName → for Cosmos
				5) connection → name of the connection string setting
			iii. What is the parameter type. This tells you what input your function receives.?
				1) Blob → InputStream
				2) Queue → String
				3) HTTP → HttpRequestMessage<Optional<String>>
				4) Cosmos → List<String> or a typed POJO
				5) Timer → String
		  How To Understand this Instantly:
				1. Look at annotation: event source
				2. See annotation properties: How to access Source
				3. See Parameter type: What data you receive
				4. Logic: What you want to do
							@FunctionName("NewFunc")
							public void run(
							    @EventHubTrigger(
							        name = "myEvent",
							        eventHubName = "logs",
							        connection = "EventHubConn"
							    ) String event,
							    ExecutionContext context
							) {
							    context.getLogger().info("Event = " + event);
							}
							
		b. Bindings:  A binding is Azure’s way of saying: “Tell me what data you need, and I’ll give it to your function automatically. 
		   You don’t need to write connection code.” Bindings are the biggest time‑saver in Azure Functions. Azure Functions have 3 types of bindings:
			i. Trigger Binding:  This is the event that starts (triggers) your function. Examples:
				• @BlobTrigger
				• @QueueTrigger
				• @TimerTrigger
				• @ServiceBusQueueTrigger
				• @CosmosDBTrigger
				• @HttpTrigger
			
			ii. Input Binding: This gives extra data to the function without you writing any code. Example in Java: 
					1) @BlobInput(name = "file", path = "images/info.json", connection = "Storage").
					2) String jsonContent
			    Azure will automatically connect to storage, read the file and give you the file content in the parameter. You don't write SDK code.
			    Types of Input Bindings:
					• @BlobInput
					• @CosmosDBInput
					• @QueueInput
					• @TableInput
					• @HttpTrigger (input + trigger)
		
			i. Output Binding: This writes data to Azure services automatically. Azure, please save this data without me using SDK. Example in Java:
					@CosmosDBOutput(	outputDoc.setValue("{\"message\": \"Hello\"}");
					    name = "out",	
					    databaseName = "Store",
					    containerName = "Audit",
					    connection = "CosmosConn"
					) OutputBinding<String> outputDoc
				Azure automatically writes to Cosmos DB. 
				Types of output Bindings:
					• @BlobOutput
					• @QueueOutput
					• @TableOutput
					• @CosmosDBOutput
					• @ServiceBusQueueOutput
					• returning an HTTP response (for HTTP trigger)
				
			  Complete example of Trigger and Bindings are:
				package com.example;
				import com.microsoft.azure.functions.*;
				import com.microsoft.azure.functions.annotation.*;
				import java.nio.charset.StandardCharsets;
				import java.util.Optional;
				public class ProcessUploadFunction {
				    /**
				     * Trigger: BlobTrigger - fires when a blob is created/updated at uploads/{imageName}
				     * Input binding #1: BlobInput - read config/settings.json (e.g., thresholds)
				     * Input binding #2: CosmosDBInput - read product metadata by id & partition key
				     * Output binding #1: CosmosDBOutput - write a processed summary document
				     * Output binding #2: QueueOutput - send a message to a queue
				     */
				    @FunctionName("ProcessUpload")
				    public void run(
				        // === Trigger (starts the function) ===
				        @BlobTrigger(
				            name = "triggerBlob",
				            path = "uploads/{imageName}",
				            connection = "StorageConn"
				        ) byte[] imageBytes,
				        // Extract {imageName} from path as a variable
				        @BindingName("imageName") String imageName,
				        // === Input Binding #1: Read a blob file as config ===
				        @BlobInput(
				            name = "configFile",
				            path = "config/settings.json",
				            connection = "StorageConn"
				        ) String configJson,
				        // === Input Binding #2: Read a Cosmos DB doc (e.g., product metadata) ===
				        @CosmosDBInput(
				            name = "product",
				            databaseName = "Store",
				            containerName = "Products",
				            partitionKey = "{imageName}",     // using imageName as partition key (example)
				            id = "{imageName}",               // using imageName as id (example)
				            connection = "CosmosConn"
				        ) Optional<String> productDoc,
				        // === Output Binding #1: Write a summary document to Cosmos DB ===
				        @CosmosDBOutput(
				            name = "processedOut",
				            databaseName = "Store",
				            containerName = "ProcessedImages",
				            connection = "CosmosConn"
				        ) OutputBinding<String> processedOut,
				        // === Output Binding #2: Send a message to a Storage Queue ===
				        @QueueOutput(
				            name = "notifyOut",
				            queueName = "image-notifications",
				            connection = "StorageConn"
				        ) OutputBinding<String> queueOut,
				        final ExecutionContext context
				    ) {
				        var log = context.getLogger();
				        log.info("Triggered by blob: " + imageName);
				        log.info("Image size: " + imageBytes.length + " bytes");
				        log.info("Config loaded (bytes): " + configJson.getBytes(StandardCharsets.UTF_8).length);
				        log.info("Product metadata present: " + productDoc.isPresent());
				        // Very basic “processing” (placeholder)
				        int width = 0;
				        int height = 0;
				        // In real code, you’d inspect imageBytes to compute metadata.
				        // Build the processed summary JSON
				        String processedJson = String.format(
				            "{ \"id\":\"%s\", \"file\":\"%s\", \"sizeBytes\":%d, \"width\":%d, \"height\":%d, \"hasProduct\":%b }",
				            imageName, imageName, imageBytes.length, width, height, productDoc.isPresent()
				        );
				        // Set outputs
				        processedOut.setValue(processedJson); // Cosmos DB output
				        String queueMsg = String.format(
				            "{ \"image\":\"%s\", \"status\":\"processed\", \"size\":%d }",
				            imageName, imageBytes.length
				        );
				        queueOut.setValue(queueMsg);          // Queue output
				        log.info("Processed document and notification queued.");
				    }
				}
			1. This function begins its execution when a new file is uploaded to Azure Blob Storage.
			   The BlobTrigger listens on the path uploads/{imageName}, and whenever a blob is created or updated at this location, the trigger fires and starts the function automatically.
			
			2. After the trigger activates, Azure Functions uses the first input binding (BlobInput) to fetch configuration details required for processing.
			   It reads the file located at config/settings.json using the StorageConn connection, and the file content is passed directly into the function without requiring any manual SDK code.
			   This configuration may include thresholds, processing rules, or other settings.
			
			3. Next, Azure uses the second input binding (CosmosDBInput) to fetch product metadata from the Products container in Cosmos DB.
			   The imageName value is used as both the document ID and partition key.
			   Azure performs the Cosmos DB lookup automatically and provides the document (if available) to the function.
			
			4. After retrieving all required data, the function performs its processing logic on the uploaded file.
			   This logic may involve analyzing the image, extracting metadata, or combining it with the configuration and product details.
			
			5. Once processing is complete, the function uses the first output binding (CosmosDBOutput) to write a summary document into the ProcessedImages container in Cosmos DB.
			   Azure handles the entire insert operation, requiring only that the function supply the JSON payload.
			
			6. Finally, the second output binding (QueueOutput) sends a small message to the image-notifications queue.
			   This message informs downstream systems that the image has been processed successfully and is ready for the next step in the workflow.
			
			7. In summary, this function demonstrates the complete Azure Functions model:
			   A trigger that starts execution, multiple input bindings that supply supporting data automatically, and multiple output bindings that deliver results without requiring any explicit client or SDK code.
			   The developer focuses solely on business logic while Azure manages all underlying connections and operations.

	1. Azure Function Authorization Levels.
	The Authorization level in Azure Functions controls who can access your function and how. It determines what kind of credentials (if any) are required when someone tries to invoke your function via HTTP. There are basically 3 levels:
		a. Anonymous: To access this no key will be required and Anyone can call the function — great for public APIs, demos, or testing
		b. Function: It require a Function Key and Only callers with the correct key can access — good for internal apps or limited sharing
		c. Admin: It require master key and Highest level — used for full control, including deployment and management
		
	2. What are the different type of API key in HTTP Trigger?
	API keys are used to authorize HTTP requests coming from browser, Postman, curl, Frontend Apps and External services. There are 3 different API key available in HTTP Trigger:
		a. default Function key: This is for single function only. This has least privilege option for invoking just this function. It is used when your function’s AuthorizationLevel is Function and you want to call this function. It is good for Integrations that call only one function endpoint.
		b. default Host key: It is used when all the function is in the same Function App. It will be used when you need one key that can call multiple functions in the same app (e.g., shared backend service or APIM calling many functions). It is still an invoke key not an admin key.
		c. Master(host) key : Admin-level for the whole Function App. It grants access to invoke endpoints and certain management endpoints. It is used when You specifically need Admin authorization, or certain management APIs. This is most powerful key so, always avoid using it in normal client calls; never embed in apps.
	If your function’s Authorization level = Anonymous → No key needed (you can leave it as-is).
	If Authorization level = Function → Choose default (function key) (recommended, least privilege).
	If Authorization level = Admin → You must choose master (host key).
	
	3. Explain Azure Time Trigger CRON Format?
	Azure Time Trigger CRON Format is: {second} {minute} {hour} {day} {month} {day-of-week}. Example:
		a. 0 */5 * * * * means run the function every 5 minutes, at second 0
			i. 0: At second 0
			ii. */5: Every 5 Min
			iii. *: Every hour
			iv. *: Every day of the month
			v. *: Every month
			vi. *: Every day of the week
		b. Every minute: 0 * * * * *
		c. Every 5 min: 0 */5 * * * *
		d. Every hour: 0 0 * * * *
		e. Every day midnight: 0 0 0 * * *
		f. Every day at 9:30: 0 30 9 * * *
		g. Every weekday at 10 AM : 0 0 10 * * 1-5
		h. Every Weekend at 11 AM: 0 0 11 * * 6,0
		i. Saturday, Sunday, and Monday at 2:00 PM: 0 0 14 * * 6,0,1
		
	4. Explain function.json
	Each function (inside a Function App) has a function.json. function.json contains a bindings array. One of the bindings is the trigger (e.g., httpTrigger, timerTrigger, queueTrigger). The trigger binding defines when the function runs and the parameter name your code receives (e.g., name: "Timer" → param($Timer)).
	A function.json does not always contain “three bindings” (trigger, input, output). 
	It contains exactly one trigger binding. 
	It can have zero or more additional input bindings.
	It can have zero or more output bindings.
	Many functions only have one binding (the trigger).
	Example: 
		a. QueueTrigger1
			
			{
			  "bindings": [
			    {
			      "name": "QueueItem",
			      "type": "queueTrigger",
			      "direction": "in",
			      "queueName": "ps-queue-items",
			      "connection": "AzureWebJobsStorage"
			    }
			  ]
			}
			
		b. TimerTrigger1
			
			{
			  "bindings": [
			    {
			      "name": "Timer",
			      "type": "timerTrigger",
			      "direction": "in",
			      "schedule": "0 */5 * * * *"
			    }
			  ]
			}
			
		c. HttpTrigger1
			
			{
			  "bindings": [
			    {
			      "authLevel": "anonymous",
			      "type": "httpTrigger",
			      "direction": "in",
			      "name": "Request",
			      "methods": [
			        "get",
			        "post"
			      ]
			    },
			    {
			      "type": "http",
			      "direction": "out",
			      "name": "Response"
			    }
			  ]
			}
	direction: "in"(Inpit binding): Azure provide data to your function. You read this data in your code and you don't create and push it yourself. Every trigger is always direction: "in" because it starts the function.
	
	direction: "out"(Output binding): Your function send data out and Azure handles writing it to the destination. You don't use SDKs; Azure does it for you.
	
	5. Explain Resource JSON → properties.config.bindings
	The bindings you see in Resource JSON → properties.config.bindings are NOT something different. They are simply Azure’s Resource management/ARM representation of the same bindings defined in function.json. So, there is only one binding definition stored with the function(function.json) and reflected into Resource JSON for management & UI.
	{
	    "id": "/subscriptions/540b7602-d4c7-41d9-ae81-9b1cfaa2f21d/resourceGroups/ADMAppDevAPACDelivery-2364176-RG/providers/Microsoft.Web/sites/VinayTestFunction/functions/TimerTrigger1",
	    "name": "VinayTestFunction/TimerTrigger1",
	    "type": "Microsoft.Web/sites/functions",
	    "location": "Central US",
	    "properties": {
	        "name": "TimerTrigger1",
	        "function_app_id": null,
	        "script_root_path_href": "https://vinaytestfunction-f3ffbqcadtgeg3g2.centralus-01.azurewebsites.net/admin/vfs/site/wwwroot/TimerTrigger1/",
	        "script_href": "https://vinaytestfunction-f3ffbqcadtgeg3g2.centralus-01.azurewebsites.net/admin/vfs/site/wwwroot/TimerTrigger1/run.ps1",
	        "config_href": "https://vinaytestfunction-f3ffbqcadtgeg3g2.centralus-01.azurewebsites.net/admin/vfs/site/wwwroot/TimerTrigger1/function.json",
	        "test_data_href": "https://vinaytestfunction-f3ffbqcadtgeg3g2.centralus-01.azurewebsites.net/admin/vfs/data/Functions/sampledata/TimerTrigger1.dat",
	        "secrets_file_href": null,
	        "href": "https://vinaytestfunction-f3ffbqcadtgeg3g2.centralus-01.azurewebsites.net/admin/functions/TimerTrigger1",
	        "config": {
	            "bindings": [
	                {
	                    "name": "Timer",
	                    "type": "timerTrigger",
	                    "direction": "in",
	                    "schedule": "0 */5 * * * *"
	                }
	            ]
	        },
	        "files": null,
	        "test_data": "",
	        "invoke_url_template": null,
	        "language": "powershell",
	        "isDisabled": false
	    }
	}
	
	6. Explain properties.isDisabled in Resource JSON.
	properties.isDisabled have two values:
		a. false: function will run
		b. true: function will never execute
	This you can achieve through Portal Disable/Enable(top toolbar) option also
		a. Enable: "isDisabled": false
		b. Disable: "isDisabled": true
	
	7. What is a Blob Trigger?
	A Blob Trigger is an Azure Function trigger that executes automatically when a blob (file) is created or updated in Azure Blob Storage. Blob trigger execute when a new file is uploaded to a container and an existing file is overwritten. It doesn't run when file is downloaded, file is deleted or file metadata is read. Common use-case is CSV/Excel processing, Image or video processing, Invoice and report injection, System-to-system file integration. 
	Let's consider one important example: 
	Blob Trigger in an ETL Pipeline: Flow: File Upload → Blob Trigger Function → Database
	Step-1) Blob container name: input-file , Uploaded file: employees.csv
		File upload trigger the Azure function. No manual API call needed
	Step-2) Transform (Processing inside Function): Inside the Blob Trigger function, four operation will be performed i.e. Read file content, Parse CSV rows, Convert data types and validate values. This operation basically include like Dat format correction if any, Removing invalid rows, Trimming Strings, Checking mandatory fields and others also depends on fields and requirements. 
	Step-3) Load (Insert into Database): After transformation, data is saved into a database like Azure SQL / Cosmos DB / PostgreSQL
	Example Function for ETL pipeline:
	
	package com.example.functions;
	import com.microsoft.azure.functions.*;
	import com.microsoft.azure.functions.annotation.*;
	import org.apache.commons.csv.CSVFormat;
	import org.apache.commons.csv.CSVParser;
	import org.apache.commons.csv.CSVRecord;
	import java.io.*;
	import java.nio.charset.StandardCharsets;
	import java.sql.*;
	import java.time.LocalDate;
	import java.time.format.DateTimeFormatter;
	import java.util.*;
	/**
	 * Blob Trigger ETL: CSV -> Transform -> Upsert to Azure SQL
	 *
	 * Expected CSV headers (case-insensitive):
	 *   EmpId, Name, JoinDate, Salary
	 *
	 * Notes:
	 * - JoinDate format expected: dd-MM-yyyy (e.g., 24-01-2026)
	 * - Salary numeric
	 *
	 * App Settings required (local.settings.json / Azure Portal -> Configuration):
	 *   DB_URL       = jdbc:sqlserver://<server>.database.windows.net:1433;database=<db>;encrypt=true;trustServerCertificate=false;loginTimeout=30;
	 *   DB_USER      = <sql-user>@<server>          (if using SQL auth)
	 *   DB_PASSWORD  = <password>                   (if using SQL auth)
	 *
	 * Storage connection (already provided by Functions runtime):
	 *   AzureWebJobsStorage
	 */
	public class EmployeeIngestFunction {
	    private static final DateTimeFormatter INPUT_DATE = DateTimeFormatter.ofPattern("dd-MM-uuuu");
	    @FunctionName("EmployeeIngest")
	    public void run(
	            @BlobTrigger(
	                name = "file",
	                dataType = "binary",
	                path = "input-files/{name}",              // <-- change to your container if needed
	                connection = "AzureWebJobsStorage"
	            ) byte[] content,
	            @BindingName("name") String fileName,
	            final ExecutionContext context) {
	        final Logger log = context.getLogger();
	        log.info("Blob Trigger fired for file: " + fileName + " (bytes: " + (content == null ? 0 : content.length) + ")");
	        if (content == null || content.length == 0) {
	            log.warning("Empty file; skipping.");
	            return;
	        }
	        // 1) Extract & Transform
	        List<EmployeeRow> validRows = new ArrayList<>();
	        List<String> badRows = new ArrayList<>();
	        try (Reader reader = new InputStreamReader(new ByteArrayInputStream(content), StandardCharsets.UTF_8);
	             CSVParser parser = CSVFormat.DEFAULT
	                     .withFirstRecordAsHeader()
	                     .withIgnoreHeaderCase()
	                     .withTrim()
	                     .withIgnoreEmptyLines()
	                     .parse(reader)) {
	            Map<String, Integer> headerMap = parser.getHeaderMap();
	            // Basic header validation
	            for (String required : Arrays.asList("EmpId", "Name", "JoinDate", "Salary")) {
	                if (!containsHeader(headerMap, required)) {
	                    throw new IllegalArgumentException("Missing required column: " + required);
	                }
	            }
	            for (CSVRecord rec : parser) {
	                try {
	                    String empIdStr = rec.get(getHeaderKey(headerMap, "EmpId"));
	                    String name = rec.get(getHeaderKey(headerMap, "Name"));
	                    String joinDateStr = rec.get(getHeaderKey(headerMap, "JoinDate"));
	                    String salaryStr = rec.get(getHeaderKey(headerMap, "Salary"));
	                    if (isBlank(empIdStr) || isBlank(name) || isBlank(joinDateStr) || isBlank(salaryStr)) {
	                        throw new IllegalArgumentException("One or more mandatory fields are blank.");
	                    }
	                    int empId = Integer.parseInt(empIdStr.trim());
	                    LocalDate joinDate = LocalDate.parse(joinDateStr.trim(), INPUT_DATE);
	                    // allow "50000" or "50000.00"
	                    double salary = Double.parseDouble(salaryStr.trim().replaceAll(",", ""));
	                    EmployeeRow row = new EmployeeRow(empId, name.trim(), joinDate, salary);
	                    validRows.add(row);
	                } catch (Exception rowEx) {
	                    badRows.add("Line " + rec.getRecordNumber() + ": " + rowEx.getMessage());
	                }
	            }
	        } catch (Exception e) {
	            log.severe("Failed to parse/transform CSV: " + e.getMessage());
	            throw new RuntimeException(e);
	        }
	        if (!badRows.isEmpty()) {
	            log.warning("Invalid rows encountered (" + badRows.size() + "):");
	            badRows.forEach(r -> log.warning("  " + r));
	            // In production, consider writing these to a dead-letter container/queue.
	        }
	        if (validRows.isEmpty()) {
	            log.info("No valid rows to load; exiting.");
	            return;
	        }
	        // 2) Load -> Azure SQL (batch upsert with MERGE)
	        String dbUrl = getEnv("DB_URL");
	        String dbUser = System.getenv("DB_USER");
	        String dbPassword = System.getenv("DB_PASSWORD");
	        if (isBlank(dbUrl)) {
	            log.severe("DB_URL is not set. Configure it in App Settings.");
	            throw new IllegalStateException("DB_URL not configured");
	        }
	        // --- Option A: SQL Authentication ---------------------------------------------------------
	        if (!isBlank(dbUser) && !isBlank(dbPassword)) {
	            try (Connection conn = DriverManager.getConnection(dbUrl, dbUser, dbPassword)) {
	                upsertEmployees(conn, validRows, log);
	            } catch (SQLException e) {
	                log.severe("SQL error: " + e.getMessage());
	                throw new RuntimeException(e);
	            }
	        } else {
	            // --- Option B: Managed Identity / AAD token -------------------------------------------
	            // Requires:
	            //  1) Function App with System-Assigned or User-Assigned Managed Identity
	            //  2) Identity granted db_datareader/db_datawriter (or custom) on the Azure SQL database
	            //  3) JDBC URL WITHOUT user/password
	            // Uncomment the block below and add azure-identity dependency if you want token-based auth.
	            /*
	            try {
	                com.azure.identity.DefaultAzureCredential credential = new com.azure.identity.DefaultAzureCredentialBuilder().build();
	                // Scope for Azure SQL is "https://database.windows.net//.default"
	                String scope = "https://database.windows.net//.default";
	                String accessToken = credential.getToken(new com.azure.core.credential.TokenRequestContext().addScopes(scope)).block().getToken();
	                Properties props = new Properties();
	                props.setProperty("accessToken", accessToken);
	                try (Connection conn = DriverManager.getConnection(dbUrl, props)) {
	                    upsertEmployees(conn, validRows, log);
	                }
	            } catch (Exception e) {
	                log.severe("AAD token-based SQL connection failed: " + e.getMessage());
	                throw new RuntimeException(e);
	            }
	            */
	            throw new IllegalStateException("No DB_USER/DB_PASSWORD provided and token-based block is disabled.");
	        }
	        log.info("ETL completed. Valid rows loaded: " + validRows.size() + "; Bad rows: " + badRows.size());
	    }
	    private static void upsertEmployees(Connection conn, List<EmployeeRow> rows, Logger log) throws SQLException {
	        // Ensure table exists (optional safeguard). For production, pre-provision schema.
	        ensureTable(conn, log);
	        // Use MERGE for upsert (insert if not exists, else update)
	        String mergeSql =
	                "MERGE dbo.Employees AS target " +
	                "USING (VALUES (?, ?, ?, ?)) AS src (EmpId, Name, JoinDate, Salary) " +
	                "ON (target.EmpId = src.EmpId) " +
	                "WHEN MATCHED THEN UPDATE SET Name = src.Name, JoinDate = src.JoinDate, Salary = src.Salary " +
	                "WHEN NOT MATCHED THEN INSERT (EmpId, Name, JoinDate, Salary) VALUES (src.EmpId, src.Name, src.JoinDate, src.Salary);";
	        conn.setAutoCommit(false);
	        int batchSize = 0;
	        try (PreparedStatement ps = conn.prepareStatement(mergeSql)) {
	            for (EmployeeRow r : rows) {
	                ps.setInt(1, r.empId);
	                ps.setString(2, r.name);
	                ps.setDate(3, Date.valueOf(r.joinDate));
	                ps.setBigDecimal(4, java.math.BigDecimal.valueOf(r.salary));
	                ps.addBatch();
	                batchSize++;
	                if (batchSize % 500 == 0) {
	                    ps.executeBatch();
	                }
	            }
	            ps.executeBatch();
	            conn.commit();
	        } catch (SQLException e) {
	            conn.rollback();
	            throw e;
	        } finally {
	            conn.setAutoCommit(true);
	        }
	    }
	    private static void ensureTable(Connection conn, Logger log) {
	        String ddl =
	            "IF NOT EXISTS (SELECT * FROM sys.objects WHERE object_id = OBJECT_ID(N'[dbo].[Employees]') AND type in (N'U')) " +
	            "BEGIN " +
	            "  CREATE TABLE [dbo].[Employees] (" +
	            "    EmpId INT NOT NULL PRIMARY KEY, " +
	            "    Name NVARCHAR(200) NOT NULL, " +
	            "    JoinDate DATE NOT NULL, " +
	            "    Salary DECIMAL(18,2) NOT NULL" +
	            "  ); " +
	            "END";
	        try (Statement st = conn.createStatement()) {
	            st.execute(ddl);
	        } catch (SQLException e) {
	            // In production, you might not want DDL here; pre-create schema instead.
	            log.warning("ensureTable() failed or table exists: " + e.getMessage());
	        }
	    }
	    // --- Helpers --------------------------------------------------------------------------------
	    private static boolean containsHeader(Map<String, Integer> headerMap, String key) {
	        return getHeaderKey(headerMap, key) != null;
	    }
	    private static String getHeaderKey(Map<String, Integer> headerMap, String key) {
	        for (String k : headerMap.keySet()) {
	            if (k.equalsIgnoreCase(key)) return k;
	        }
	        return null;
	    }
	    private static boolean isBlank(String s) {
	        return s == null || s.trim().isEmpty();
	    }
	    // DTO for transformed data
	    private static class EmployeeRow {
	        final int empId;
	        final String name;
	        final LocalDate joinDate;
	        final double salary;
	        EmployeeRow(int empId, String name, LocalDate joinDate, double salary) {
	            this.empId = empId;
	            this.name = name;
	            this.joinDate = joinDate;
	            this.salary = salary;
	        }
	    }
	}
	
	8. What is a Cosmos DB Trigger?
	A Cosmos DB Trigger listens to the Cosmos DB Change Feed. The Change Feed: Tracks new and updated documents, Preserves order, Is reliable and durable. Your function reacts to those document changes. Operation trigger when new document is inserted and existing document is updated only.
	A Cosmos DB Trigger works only with Azure Cosmos DB because: It is built on Cosmos DB Change Feed, Change Feed exists only in Cosmos DB,Other databases do not expose this mechanism in the same way . If you are not using Cosmos DB, this trigger is NOT applicable.
	
	9. Kafka Trigger vs Kafka Output
		a. Kafka Trigger: It means Function act as a Kafka Consumer. A function that will be run whenever a message is added to a specified Kafka topic. Use Kafka Trigger when Kafka already has data/events and your function needs to react to those events. 
			i. Usecase: Consume events from Microservices, Process streaming data, validate on enrich messages, store kafka events into DB, Trigger alerts/logging.
		b. Kafka Output: It means Function act as a Kafka Producer. Function executes first (triggered by HTTP, Blob, Cosmos DB, etc.). Flow: 
		Some trigger (HTTP / Blob / Timer / Kafka)---> Azure Functions Runs---> Function sends message to Kafka topic
		Usecase: Your function needs to publish events, Kafka is part of your event pipeline.
		
	10. MySql Input Binding vs MySql Output Binding vs MySql Trigger in Azure Function?
		a. MySql Input Binding: A function that will be run on an HTTP trigger and returns the results of a provided MySQL query.
		b. MySql Output Binding: MySqlOutputBinding_description
		c. MySql Trigger: A function will be invoked when there are changes detected on a MySQL table on which the trigger is configured.
	11. SQL Input Binding vs SQL Output Binding vs SQL Trigger in Azure Function?
		a. SQL Input Binding: A function that will be run on an HTTP trigger and returns the results of a provided SQL query.
		b. SQL Output Binding: A function that will be run on an HTTP trigger and takes a list of rows and upserts them into the user table (i.e. If a row doesn't already exist, it is added. If it does, it is updated).
		c. SQL Trigger: A function that will be invoked when there are changes detected on a SQL table on which the trigger is configured.
		
